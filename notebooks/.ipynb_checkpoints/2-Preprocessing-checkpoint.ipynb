{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19229f8e",
   "metadata": {},
   "source": [
    "# 2 - Preprocessing <a class=\"anchor\" id=\"top\"></a>\n",
    "* [Introduction](#intro)\n",
    "* [Setup](#setup)\n",
    "* [Loading the data](#load-data)\n",
    "* [Feature engineering](#ft-eng)\n",
    "    * [Target variable](#target)\n",
    "    * [Categorical variables](#cat)\n",
    "    * [Continuous variables](#continuous)\n",
    "* [Train-test split](#train-test)\n",
    "* [Define and fit pipeline](#pipeline)\n",
    "* [Transform and write data](#write-s3)\n",
    "* [Serialize and store pipelines](#store-pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189b4abb",
   "metadata": {},
   "source": [
    "## Introduction <a class=\"anchor\" id=\"intro\"></a>\n",
    "In this section, we preform all neccesary data operations to prepare our data for use in models.\n",
    "This includes filtering, feature subsetting, feature engineering, cleaning, and finally exporting to S3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc82af5",
   "metadata": {},
   "source": [
    "## Setup <a class=\"anchor\" id=\"setup\"></a>\n",
    "First, we must import relevant Spark modules as well as libraries for statistical analysis and visualizations.\n",
    "Note that will also start the Spark application that creates the `SparkSession` and sets it to the `spark` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ec89b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cleanup -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d38480b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark\n",
    "import pyspark.ml as ml\n",
    "import pyspark.sql as sql\n",
    "import pyspark.sql.types as types\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import boto3\n",
    "import mleap\n",
    "import zipfile\n",
    "import tarfile\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffbd7df",
   "metadata": {},
   "source": [
    "## Loading the data <a class=\"anchor\" id=\"load-data\"></a>\n",
    "\n",
    "Now, we load the airline data from S3.\n",
    "To do this, we must first get the bucket name where the data is stored.\n",
    "This variable is stored on the local Sagemaker notebook instance during the `DevEnvironment` stack creation \n",
    "and must be explicitly passed to the Spark cluster.\n",
    "\n",
    "Additionally, the data is also subsetted and filter before preforming any feature engineering.\n",
    "This is to reduce the number of features we are working with \n",
    "and ensure that the features we have are meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3d3eaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import json\n",
    "with open(\"/home/ec2-user/.aiml-bb/stack-data.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    data_bucket = data[\"data_bucket\"]\n",
    "    model_bucket = data[\"model_bucket\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "921f5ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully passed 'data_bucket' as 'data_bucket' to Spark kernel"
     ]
    }
   ],
   "source": [
    "%%send_to_spark -i data_bucket -t str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31189d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully passed 'model_bucket' as 'model_bucket' to Spark kernel"
     ]
    }
   ],
   "source": [
    "%%send_to_spark -i model_bucket -t str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6104a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Subset ETL output to only relevant columns for model input.\n",
    "join_df = spark.read.parquet(f\"s3://{data_bucket}/etl_output/joined_airline_weather_data/\")\n",
    "\n",
    "# Work on a subset of the data that reflects the input schema \n",
    "# we'd like our endpoint to recieve.\n",
    "input_df = (\n",
    "    join_df\n",
    "    # Drop all other NaN values.\n",
    "    .dropna(\n",
    "        subset=[\"origin\", \"dest\", \"origin_station_id\", \"dest_station_id\"]\n",
    "    )\n",
    "    # Fill NaN values where neccesary.\n",
    "    .na.fill(\n",
    "        0.0,\n",
    "        subset=[\n",
    "            \"arr_delay\",\n",
    "            \"origin_prcp\", \"origin_snow\", \"origin_snwd\",\n",
    "            \"dest_prcp\", \"dest_snow\", \"dest_snwd\"\n",
    "        ]\n",
    "    )\n",
    "    # Make feature selection.\n",
    "    .select(\n",
    "        \"arr_delay\", \"cancelled\", \"diverted\",\n",
    "        \"op_carrier\",\n",
    "        \"origin\", \"origin_latitude\", \"origin_longitude\",\n",
    "        \"dest\", \"dest_latitude\", \"dest_longitude\",\n",
    "        \"origin_tmax\", \"origin_tmin\", \n",
    "        \"origin_prcp\", \"origin_snow\", \"origin_snwd\",\n",
    "        \"dest_tmax\", \"dest_tmin\", \n",
    "        \"dest_prcp\", \"dest_snow\", \"dest_snwd\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1dccf89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>arr_delay</th><th>cancelled</th><th>diverted</th><th>op_carrier</th><th>origin</th><th>origin_latitude</th><th>origin_longitude</th><th>dest</th><th>dest_latitude</th><th>dest_longitude</th><th>origin_tmax</th><th>origin_tmin</th><th>origin_prcp</th><th>origin_snow</th><th>origin_snwd</th><th>dest_tmax</th><th>dest_tmin</th><th>dest_prcp</th><th>dest_snow</th><th>dest_snwd</th></tr><tr><td>-9.0</td><td>0.0</td><td>0.0</td><td>EV</td><td>ATL</td><td>33.6367</td><td>-84.428101</td><td>ABE</td><td>40.652099609375</td><td>-75.44080352783203</td><td>178.0</td><td>94.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>172.0</td><td>133.0</td><td>81.0</td><td>0.0</td><td>0.0</td></tr><tr><td>213.0</td><td>0.0</td><td>0.0</td><td>EV</td><td>ATL</td><td>33.6367</td><td>-84.428101</td><td>ABE</td><td>40.652099609375</td><td>-75.44080352783203</td><td>244.0</td><td>194.0</td><td>462.0</td><td>0.0</td><td>0.0</td><td>289.0</td><td>228.0</td><td>13.0</td><td>0.0</td><td>0.0</td></tr><tr><td>73.0</td><td>0.0</td><td>0.0</td><td>EV</td><td>ATL</td><td>33.6367</td><td>-84.428101</td><td>ABE</td><td>40.652099609375</td><td>-75.44080352783203</td><td>244.0</td><td>194.0</td><td>462.0</td><td>0.0</td><td>0.0</td><td>289.0</td><td>228.0</td><td>13.0</td><td>0.0</td><td>0.0</td></tr><tr><td>48.0</td><td>0.0</td><td>0.0</td><td>EV</td><td>ATL</td><td>33.6367</td><td>-84.428101</td><td>ABE</td><td>40.652099609375</td><td>-75.44080352783203</td><td>244.0</td><td>194.0</td><td>462.0</td><td>0.0</td><td>0.0</td><td>289.0</td><td>228.0</td><td>13.0</td><td>0.0</td><td>0.0</td></tr><tr><td>106.0</td><td>0.0</td><td>0.0</td><td>9E</td><td>ATL</td><td>33.6367</td><td>-84.428101</td><td>ABE</td><td>40.652099609375</td><td>-75.44080352783203</td><td>106.0</td><td>50.0</td><td>254.0</td><td>0.0</td><td>0.0</td><td>11.0</td><td>-21.0</td><td>58.0</td><td>0.0</td><td>30.0</td></tr></table><br /><pre>only showing top 5 rows</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%pretty\n",
    "input_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ab223a",
   "metadata": {},
   "source": [
    "## Feature engineering <a class=\"anchor\" id=\"ft-eng\"></a>\n",
    "We now define the feature engineering steps that will make up our preprocessing `Pipeline`.\n",
    "Note that all steps must be either a Spark `Transformer` or `Estimator` to be able to be serialized by MLeap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af01e860",
   "metadata": {},
   "source": [
    "### Target variable <a class=\"anchor\" id=\"target\"></a>\n",
    "We will reduce the regression problem of determining arrival delay into the binary classfication problem or detmering if a delay (or cancellation) will occur.\n",
    "Because this step will not be a part of our inference pipeline, we do not need to wrap it in a Spark `Transformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "092895d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the new categorical column we want to predict.\n",
    "# Strings are used here so we can postprocess the binary \n",
    "# classification to readable results. \n",
    "input_df = (\n",
    "    input_df\n",
    "    .withColumn(\n",
    "        \"delay_status\", \n",
    "        F.when(\n",
    "            (F.col(\"arr_delay\") > 15) \n",
    "                | (F.col(\"cancelled\") == 1) \n",
    "                | (F.col(\"diverted\") == 1), \n",
    "            \"Delayed\"\n",
    "        )\n",
    "        .otherwise(\"OnTime\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9bbee3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now encode target and apply to input data.\n",
    "target_indexer = ml.feature.StringIndexer(inputCol=\"delay_status\", outputCol=\"target\")\n",
    "target_indexer_model = target_indexer.fit(input_df)\n",
    "input_df = target_indexer_model.transform(input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "320e986e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a decoder to be used in postprocessing.\n",
    "target_idx_decoder = ml.feature.IndexToString(inputCol=\"target\", outputCol=\"delay_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4300ff1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>arr_delay</th><th>cancelled</th><th>diverted</th><th>delay_status</th><th>target</th></tr><tr><td>39.0</td><td>0.0</td><td>0.0</td><td>Delayed</td><td>1.0</td></tr><tr><td>-8.0</td><td>0.0</td><td>0.0</td><td>OnTime</td><td>0.0</td></tr><tr><td>-8.0</td><td>0.0</td><td>0.0</td><td>OnTime</td><td>0.0</td></tr><tr><td>-4.0</td><td>0.0</td><td>0.0</td><td>OnTime</td><td>0.0</td></tr><tr><td>-21.0</td><td>0.0</td><td>0.0</td><td>OnTime</td><td>0.0</td></tr></table><br /><pre>only showing top 5 rows</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%pretty\n",
    "input_df.select(\"arr_delay\", \"cancelled\", \"diverted\", \"delay_status\", \"target\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed2f34d",
   "metadata": {},
   "source": [
    "### Categorical variables <a class=\"anchor\" id=\"cat\"></a>\n",
    "Here, we will encode the relevant categorical variables to numeric types. \n",
    "For this data set, this means we must encode the carrier as well as the origin and destination airport information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65c89738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "carrier_indexer = ml.feature.StringIndexer(inputCol=\"op_carrier\", outputCol=\"carrier_idx\")\n",
    "carrier_encoder = ml.feature.OneHotEncoderEstimator(inputCols=[\"carrier_idx\"], outputCols=[\"carrier_ohe\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d255eb63",
   "metadata": {},
   "source": [
    "Because there are ~300 airports, and we need to account for both origin and destination,\n",
    "one hot encoding would create too many features.\n",
    "Airports do appear in geographic clusters though, for example around urban locations.\n",
    "So, to encode airports, we'll cluster the latitudes and longitudes using K-Means.\n",
    "Note that we will only preform this search on the origin airports because of the similarities in the distribution of origin and destination airports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6b89865f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vectorize geolocation for use in clustering.\n",
    "origin_geo_assembler = ml.feature.VectorAssembler(\n",
    "    inputCols=[\"origin_latitude\", \"origin_longitude\"], \n",
    "    outputCol=\"origin_geo_features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "kmeans_eval_df = origin_geo_assembler.transform(input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9ef227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d17c3a34841b46419c3fbd19192329ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Search for an optimal K.\n",
    "# silhouette_scores = []\n",
    "evaluator = ml.evaluation.ClusteringEvaluator(\n",
    "    metricName=\"silhouette\", \n",
    "    distanceMeasure=\"squaredEuclidean\",\n",
    "    featuresCol=\"origin_geo_features\",\n",
    "    predictionCol=\"origin_geo_cluster\"\n",
    ")\n",
    "\n",
    "for k in range(20, 30):\n",
    "    candidate_kmeans = ml.clustering.KMeans(\n",
    "        k=k,\n",
    "        featuresCol=\"origin_geo_features\",\n",
    "        predictionCol=\"origin_geo_cluster\"\n",
    "    )\n",
    "    candidate_kmeans_model = candidate_kmeans.fit(kmeans_eval_df)\n",
    "    transformed_kmeans_eval_df = candidate_kmeans_model.transform(kmeans_eval_df)\n",
    "\n",
    "    evaluation_score = evaluator.evaluate(transformed_kmeans_eval_df)\n",
    "    silhouette_scores.append(evaluation_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ce9af37d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7935991349235418, 0.7951541999640632, 0.7962580427641034, 0.6208395707571753, 0.5181488912024634, 0.581405617511063, 0.6719085293109444, 0.6588834486946222, 0.7117989190279977, 0.7185847094251225, 0.5679515291075474, 0.602948717944024, 0.6165610818226945, 0.6303616774089789, 0.6488521893517488, 0.685676735367946, 0.7330370289036479, 0.753958241456658]"
     ]
    }
   ],
   "source": [
    "silhouette_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187b3fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model results.\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(2, 20), silhouette_scores)\n",
    "ax.set_xlabel(\"Number of clusters\")\n",
    "ax.set_ylabel(\"Silhouette score\")\n",
    "ax.set_title(\"optimal clustering parameters for airport geolocations\")\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a0c3cf",
   "metadata": {},
   "source": [
    "While it isn't e can see that 6 is near the elbow of the plot and will be our choice for K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bf7bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create finalized assemblers and clusterers for the \n",
    "# preprocessing pipeline.\n",
    "origin_geo_assembler = ml.feature.VectorAssembler(\n",
    "    inputCols=[\"origin_latitude\", \"origin_longitude\"], \n",
    "    outputCol=\"origin_geo_features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "dest_geo_assembler = ml.feature.VectorAssembler(\n",
    "    inputCols=[\"dest_latitude\", \"dest_longitude\"], \n",
    "    outputCol=\"dest_geo_features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "origin_geo_kmeans = ml.clustering.KMeans(\n",
    "        k=6,\n",
    "        featuresCol=\"origin_geo_features\",\n",
    "        predictionCol=\"origin_geo_cluster\"\n",
    ")\n",
    "dest_geo_kmeans = ml.clustering.KMeans(\n",
    "        k=6,\n",
    "        featuresCol=\"dest_geo_features\",\n",
    "        predictionCol=\"dest_geo_cluster\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4a9440",
   "metadata": {},
   "source": [
    "### Continuous variables <a class=\"anchor\" id=\"continuous\"></a>\n",
    "The continuous variables in our dataset include only the weather data. \n",
    "We will be using standardization for all metrics, because weather data is subject to meaningful outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f3c1a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Collect all weather columns so standardization can \n",
    "# be preformed on all weather features at once.\n",
    "weather_cols = [\n",
    "    \"origin_tmax\", \"origin_tmin\", \"origin_prcp\", \"origin_snow\", \"origin_snwd\",\n",
    "    \"dest_tmax\", \"dest_tmin\", \"dest_prcp\", \"dest_snow\", \"dest_snwd\"\n",
    "]\n",
    "weather_col_assembler = ml.feature.VectorAssembler(\n",
    "    inputCols=weather_cols, \n",
    "    outputCol=\"weather_features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "weather_std_scalar = ml.feature.StandardScaler(\n",
    "    inputCol=\"weather_features\", \n",
    "    outputCol=\"weather_features_scaled\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d801459f",
   "metadata": {},
   "source": [
    "## Define and fit pipeline <a class=\"anchor\" id=\"pipeline\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "559f052f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Collect all the generated features into a single vector.\n",
    "# THis will be the last stage in the pipeline.\n",
    "feature_assembler = ml.feature.VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"carrier_ohe\",\n",
    "        \"origin_freq\",\n",
    "        \"dest_freq\",\n",
    "        \"origin_state_ohe\",\n",
    "        \"dest_state_ohe\",\n",
    "        \"weather_features_scaled\"\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11d01dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define Pipeline object.\n",
    "preprocess_pipeline = ml.Pipeline(\n",
    "    stages=[\n",
    "        carrier_indexer,\n",
    "        carrier_encoder,\n",
    "        origin_geo_assembler,\n",
    "        dest_geo_assembler,\n",
    "        origin_geo_kmeans,\n",
    "        dest_geo_kmeans,\n",
    "        weather_col_assembler,\n",
    "        weather_std_scalar,\n",
    "        feature_assembler,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d08ff08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the preprocess model and transform the input data.\n",
    "# Note that the transformed data is cached to avoid recomputation \n",
    "# when writing partitions (see below).\n",
    "preprocess_model = preprocess_pipeline.fit(input_df)\n",
    "transformed_input_df = preprocess_model.transform(input_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aac70a6",
   "metadata": {},
   "source": [
    "## Partition and write data <a class=\"anchor\" id=\"write-s3\"></a>\n",
    "Partitition input data into train, validation, and test sets and write out files to CSVs in S3 for training models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "458d76ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Spliting in train and test set. Note that this operation \n",
    "# sorts the data set.\n",
    "(train_df, validation_df, test_df) = transformed_input_df.randomSplit([0.75, 0.15, 0.10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac708a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>arr_delay</th><th>cancelled</th><th>diverted</th><th>op_carrier</th><th>origin</th><th>origin_state_abr</th><th>dest</th><th>dest_state_abr</th><th>origin_tmax</th><th>origin_tmin</th><th>origin_prcp</th><th>origin_snow</th><th>origin_snwd</th><th>dest_tmax</th><th>dest_tmin</th><th>dest_prcp</th><th>dest_snow</th><th>dest_snwd</th><th>delay_status</th><th>target</th><th>carrier_idx</th><th>carrier_ohe</th><th>origin_freq</th><th>dest_freq</th><th>origin_state_idx</th><th>dest_state_idx</th><th>origin_state_ohe</th><th>dest_state_ohe</th><th>weather_features</th><th>weather_features_scaled</th><th>features</th></tr><tr><td>-63.0</td><td>0.0</td><td>0.0</td><td>OO</td><td>SFO</td><td>CA</td><td>GEG</td><td>WA</td><td>183.0</td><td>106.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>72.0</td><td>-5.0</td><td>28.0</td><td>8.0</td><td>0.0</td><td>OnTime</td><td>0.0</td><td>3.0</td><td>(22,[3],[1.0])</td><td>0.02584784579948182</td><td>0.001903570441812...</td><td>0.0</td><td>12.0</td><td>(52,[0],[1.0])</td><td>(52,[12],[1.0])</td><td>[183.0,106.0,0.0,...</td><td>[1.76520170281059...</td><td>(138,[3,22,23,24,...</td></tr><tr><td>-58.0</td><td>0.0</td><td>0.0</td><td>DL</td><td>MSP</td><td>MN</td><td>GEG</td><td>WA</td><td>-10.0</td><td>-49.0</td><td>0.0</td><td>0.0</td><td>50.0</td><td>-5.0</td><td>-43.0</td><td>0.0</td><td>0.0</td><td>150.0</td><td>OnTime</td><td>0.0</td><td>1.0</td><td>(22,[1],[1.0])</td><td>0.021160588142142344</td><td>0.001903570441812...</td><td>13.0</td><td>12.0</td><td>(52,[13],[1.0])</td><td>(52,[12],[1.0])</td><td>[-10.0,-49.0,0.0,...</td><td>[-0.0964591094432...</td><td>(138,[1,22,23,37,...</td></tr><tr><td>-56.0</td><td>0.0</td><td>0.0</td><td>DL</td><td>MSP</td><td>MN</td><td>GEG</td><td>WA</td><td>28.0</td><td>11.0</td><td>23.0</td><td>0.0</td><td>30.0</td><td>61.0</td><td>-32.0</td><td>0.0</td><td>0.0</td><td>150.0</td><td>OnTime</td><td>0.0</td><td>1.0</td><td>(22,[1],[1.0])</td><td>0.021160588142142344</td><td>0.001903570441812...</td><td>13.0</td><td>12.0</td><td>(52,[13],[1.0])</td><td>(52,[12],[1.0])</td><td>[28.0,11.0,23.0,0...</td><td>[0.27008550644096...</td><td>(138,[1,22,23,37,...</td></tr><tr><td>-55.0</td><td>0.0</td><td>0.0</td><td>UA</td><td>ORD</td><td>IL</td><td>GEG</td><td>WA</td><td>44.0</td><td>-5.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>83.0</td><td>6.0</td><td>91.0</td><td>0.0</td><td>0.0</td><td>OnTime</td><td>0.0</td><td>4.0</td><td>(22,[4],[1.0])</td><td>0.047518996948989554</td><td>0.001903570441812...</td><td>3.0</td><td>12.0</td><td>(52,[3],[1.0])</td><td>(52,[12],[1.0])</td><td>(10,[0,1,5,6,7],[...</td><td>(10,[0,1,5,6,7],[...</td><td>(138,[4,22,23,27,...</td></tr><tr><td>-54.0</td><td>0.0</td><td>0.0</td><td>DL</td><td>MSP</td><td>MN</td><td>GEG</td><td>WA</td><td>-10.0</td><td>-49.0</td><td>0.0</td><td>0.0</td><td>50.0</td><td>-5.0</td><td>-43.0</td><td>0.0</td><td>0.0</td><td>150.0</td><td>OnTime</td><td>0.0</td><td>1.0</td><td>(22,[1],[1.0])</td><td>0.021160588142142344</td><td>0.001903570441812...</td><td>13.0</td><td>12.0</td><td>(52,[13],[1.0])</td><td>(52,[12],[1.0])</td><td>[-10.0,-49.0,0.0,...</td><td>[-0.0964591094432...</td><td>(138,[1,22,23,37,...</td></tr></table><br /><pre>only showing top 5 rows</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%pretty\n",
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1309c44f",
   "metadata": {},
   "source": [
    "The writing operation takes a while. \n",
    "It's recommended to increase the number of core nodes in the cluster at this point, if possible.\n",
    "Alternatively, this code can be run in a Glue Job that transforms the data,\n",
    "writes the partitions to S3, \n",
    "then serializes and saves the processing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16fd4395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save transformed training data to CSV in S3 by converting to RDD.\n",
    "train_lines = train_df.rdd.map(\n",
    "    lambda row: str(row.target) + \",\" + \",\".join(map(str, row.features.toArray()))\n",
    ")\n",
    "train_lines.saveAsTextFile(f\"s3a://{model_bucket}/preprocessing_output/train/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "306ad3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save transformed training data to CSV in S3 by converting to RDD.\n",
    "validation_lines = validation_df.rdd.map(\n",
    "    lambda row: str(row.target) + \",\" + \",\".join(map(str, row.features.toArray()))\n",
    ")\n",
    "validation_lines.saveAsTextFile(f\"s3a://{model_bucket}/preprocessing_output/validation/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1891f96d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Repeat the same saving method for the test data.\n",
    "test_lines = test_df.rdd.map(\n",
    "    lambda row: str(row.target) + \",\" + \",\".join(map(str, row.features.toArray()))\n",
    ")\n",
    "test_lines.saveAsTextFile(f\"s3a://{model_bucket}/preprocessing_output/test/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24370dcd",
   "metadata": {},
   "source": [
    "## Serialize and store pipelines <a class=\"anchor\" id=\"store-pipeline\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9cd493e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import boto3\n",
    "from mleap.pyspark.spark_support import SimpleSparkSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4dce85dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o651.serializeToBundle.\n",
      ": java.util.NoSuchElementException: key not found: org.apache.spark.ml.feature.SQLTransformer\n",
      "\tat scala.collection.MapLike$class.default(MapLike.scala:228)\n",
      "\tat scala.collection.AbstractMap.default(Map.scala:59)\n",
      "\tat scala.collection.MapLike$class.apply(MapLike.scala:141)\n",
      "\tat scala.collection.AbstractMap.apply(Map.scala:59)\n",
      "\tat ml.combust.bundle.BundleRegistry.opForObj(BundleRegistry.scala:102)\n",
      "\tat ml.combust.bundle.serializer.GraphSerializer$$anonfun$writeNode$1.apply(GraphSerializer.scala:31)\n",
      "\tat ml.combust.bundle.serializer.GraphSerializer$$anonfun$writeNode$1.apply(GraphSerializer.scala:30)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat ml.combust.bundle.serializer.GraphSerializer.writeNode(GraphSerializer.scala:30)\n",
      "\tat ml.combust.bundle.serializer.GraphSerializer$$anonfun$write$2.apply(GraphSerializer.scala:21)\n",
      "\tat ml.combust.bundle.serializer.GraphSerializer$$anonfun$write$2.apply(GraphSerializer.scala:21)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\n",
      "\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)\n",
      "\tat ml.combust.bundle.serializer.GraphSerializer.write(GraphSerializer.scala:20)\n",
      "\tat org.apache.spark.ml.bundle.ops.PipelineOp$$anon$1.store(PipelineOp.scala:21)\n",
      "\tat org.apache.spark.ml.bundle.ops.PipelineOp$$anon$1.store(PipelineOp.scala:14)\n",
      "\tat ml.combust.bundle.serializer.ModelSerializer$$anonfun$write$1.apply(ModelSerializer.scala:87)\n",
      "\tat ml.combust.bundle.serializer.ModelSerializer$$anonfun$write$1.apply(ModelSerializer.scala:83)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat ml.combust.bundle.serializer.ModelSerializer.write(ModelSerializer.scala:83)\n",
      "\tat ml.combust.bundle.serializer.NodeSerializer$$anonfun$write$1.apply(NodeSerializer.scala:85)\n",
      "\tat ml.combust.bundle.serializer.NodeSerializer$$anonfun$write$1.apply(NodeSerializer.scala:81)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat ml.combust.bundle.serializer.NodeSerializer.write(NodeSerializer.scala:81)\n",
      "\tat ml.combust.bundle.serializer.BundleSerializer$$anonfun$write$1.apply(BundleSerializer.scala:34)\n",
      "\tat ml.combust.bundle.serializer.BundleSerializer$$anonfun$write$1.apply(BundleSerializer.scala:29)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat ml.combust.bundle.serializer.BundleSerializer.write(BundleSerializer.scala:29)\n",
      "\tat ml.combust.bundle.BundleWriter.save(BundleWriter.scala:31)\n",
      "\tat ml.combust.mleap.spark.SimpleSparkSerializer$$anonfun$serializeToBundleWithFormat$2.apply(SimpleSparkSerializer.scala:26)\n",
      "\tat ml.combust.mleap.spark.SimpleSparkSerializer$$anonfun$serializeToBundleWithFormat$2.apply(SimpleSparkSerializer.scala:25)\n",
      "\tat resource.AbstractManagedResource$$anonfun$5.apply(AbstractManagedResource.scala:88)\n",
      "\tat scala.util.control.Exception$Catch$$anonfun$either$1.apply(Exception.scala:125)\n",
      "\tat scala.util.control.Exception$Catch$$anonfun$either$1.apply(Exception.scala:125)\n",
      "\tat scala.util.control.Exception$Catch.apply(Exception.scala:103)\n",
      "\tat scala.util.control.Exception$Catch.either(Exception.scala:125)\n",
      "\tat resource.AbstractManagedResource.acquireFor(AbstractManagedResource.scala:88)\n",
      "\tat resource.ManagedResourceOperations$class.apply(ManagedResourceOperations.scala:26)\n",
      "\tat resource.AbstractManagedResource.apply(AbstractManagedResource.scala:50)\n",
      "\tat resource.DeferredExtractableManagedResource$$anonfun$tried$1.apply(AbstractManagedResource.scala:33)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat resource.DeferredExtractableManagedResource.tried(AbstractManagedResource.scala:33)\n",
      "\tat ml.combust.mleap.spark.SimpleSparkSerializer.serializeToBundleWithFormat(SimpleSparkSerializer.scala:27)\n",
      "\tat ml.combust.mleap.spark.SimpleSparkSerializer.serializeToBundle(SimpleSparkSerializer.scala:17)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/mleap/pyspark/spark_support.py\", line 42, in serializeToBundle\n",
      "    self._java_obj.serializeToBundle(transformer._to_java(), path, dataset._jdf)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o651.serializeToBundle.\n",
      ": java.util.NoSuchElementException: key not found: org.apache.spark.ml.feature.SQLTransformer\n",
      "\tat scala.collection.MapLike$class.default(MapLike.scala:228)\n",
      "\tat scala.collection.AbstractMap.default(Map.scala:59)\n",
      "\tat scala.collection.MapLike$class.apply(MapLike.scala:141)\n",
      "\tat scala.collection.AbstractMap.apply(Map.scala:59)\n",
      "\tat ml.combust.bundle.BundleRegistry.opForObj(BundleRegistry.scala:102)\n",
      "\tat ml.combust.bundle.serializer.GraphSerializer$$anonfun$writeNode$1.apply(GraphSerializer.scala:31)\n",
      "\tat ml.combust.bundle.serializer.GraphSerializer$$anonfun$writeNode$1.apply(GraphSerializer.scala:30)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat ml.combust.bundle.serializer.GraphSerializer.writeNode(GraphSerializer.scala:30)\n",
      "\tat ml.combust.bundle.serializer.GraphSerializer$$anonfun$write$2.apply(GraphSerializer.scala:21)\n",
      "\tat ml.combust.bundle.serializer.GraphSerializer$$anonfun$write$2.apply(GraphSerializer.scala:21)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\n",
      "\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)\n",
      "\tat ml.combust.bundle.serializer.GraphSerializer.write(GraphSerializer.scala:20)\n",
      "\tat org.apache.spark.ml.bundle.ops.PipelineOp$$anon$1.store(PipelineOp.scala:21)\n",
      "\tat org.apache.spark.ml.bundle.ops.PipelineOp$$anon$1.store(PipelineOp.scala:14)\n",
      "\tat ml.combust.bundle.serializer.ModelSerializer$$anonfun$write$1.apply(ModelSerializer.scala:87)\n",
      "\tat ml.combust.bundle.serializer.ModelSerializer$$anonfun$write$1.apply(ModelSerializer.scala:83)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat ml.combust.bundle.serializer.ModelSerializer.write(ModelSerializer.scala:83)\n",
      "\tat ml.combust.bundle.serializer.NodeSerializer$$anonfun$write$1.apply(NodeSerializer.scala:85)\n",
      "\tat ml.combust.bundle.serializer.NodeSerializer$$anonfun$write$1.apply(NodeSerializer.scala:81)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat ml.combust.bundle.serializer.NodeSerializer.write(NodeSerializer.scala:81)\n",
      "\tat ml.combust.bundle.serializer.BundleSerializer$$anonfun$write$1.apply(BundleSerializer.scala:34)\n",
      "\tat ml.combust.bundle.serializer.BundleSerializer$$anonfun$write$1.apply(BundleSerializer.scala:29)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat ml.combust.bundle.serializer.BundleSerializer.write(BundleSerializer.scala:29)\n",
      "\tat ml.combust.bundle.BundleWriter.save(BundleWriter.scala:31)\n",
      "\tat ml.combust.mleap.spark.SimpleSparkSerializer$$anonfun$serializeToBundleWithFormat$2.apply(SimpleSparkSerializer.scala:26)\n",
      "\tat ml.combust.mleap.spark.SimpleSparkSerializer$$anonfun$serializeToBundleWithFormat$2.apply(SimpleSparkSerializer.scala:25)\n",
      "\tat resource.AbstractManagedResource$$anonfun$5.apply(AbstractManagedResource.scala:88)\n",
      "\tat scala.util.control.Exception$Catch$$anonfun$either$1.apply(Exception.scala:125)\n",
      "\tat scala.util.control.Exception$Catch$$anonfun$either$1.apply(Exception.scala:125)\n",
      "\tat scala.util.control.Exception$Catch.apply(Exception.scala:103)\n",
      "\tat scala.util.control.Exception$Catch.either(Exception.scala:125)\n",
      "\tat resource.AbstractManagedResource.acquireFor(AbstractManagedResource.scala:88)\n",
      "\tat resource.ManagedResourceOperations$class.apply(ManagedResourceOperations.scala:26)\n",
      "\tat resource.AbstractManagedResource.apply(AbstractManagedResource.scala:50)\n",
      "\tat resource.DeferredExtractableManagedResource$$anonfun$tried$1.apply(AbstractManagedResource.scala:33)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat resource.DeferredExtractableManagedResource.tried(AbstractManagedResource.scala:33)\n",
      "\tat ml.combust.mleap.spark.SimpleSparkSerializer.serializeToBundleWithFormat(SimpleSparkSerializer.scala:27)\n",
      "\tat ml.combust.mleap.spark.SimpleSparkSerializer.serializeToBundle(SimpleSparkSerializer.scala:17)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "serializer = mleap.pyspark.spark_support.SimpleSparkSerializer()\n",
    "serializer.serializeToBundle(\n",
    "    preprocess_model, \n",
    "    \"jar:file:/tmp/preprocess_model.zip\", \n",
    "    transformed_input_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6080cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09d225c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"s3://{model_bucket}/spark-preprocessor/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b54068e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
