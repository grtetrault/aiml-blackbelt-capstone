{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "382eeacc",
   "metadata": {},
   "source": [
    "# 4 - Endpoint <a class=\"anchor\" id=\"top\"></a>\n",
    "* [Introduction](#intro)\n",
    "* [Setup](#setup)\n",
    "* [Define and deploy the model](#define)\n",
    "* [Productionize the endpoint](#prod)\n",
    "    * [Autoscaling](#autoscale)\n",
    "    * [Model monitor](#monitor)\n",
    "* [Testing the endpoint](#test)\n",
    "* [Cleanup resources](#cleanup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df92ea3",
   "metadata": {},
   "source": [
    "## Introduction <a class=\"anchor\" id=\"intro\"></a>\n",
    "In this last section, we create a Sagemaker endpoint to allow for real-time predictions using our trained models.\n",
    "After creating the endpoint, we will test a simple application that takes in basic flight information and returns \n",
    "the models prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627d08ce",
   "metadata": {},
   "source": [
    "## Setup <a class=\"anchor\" id=\"setup\"></a>\n",
    "First, we import Sageamker SDK dependencies as well as modules used in application below.\n",
    "We also get relevant sessions and read in local environment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f54870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml\n",
    "import json\n",
    "import uuid\n",
    "import boto3\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import dateutil.parser\n",
    "import sagemaker as sm\n",
    "import sagemaker.sparkml as sparkml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f48d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get relevant sessions.\n",
    "sm_session = sm.Session()\n",
    "role = sm.get_execution_role()\n",
    "boto3_session = boto3.session.Session()\n",
    "now = dt.datetime.now().strftime(r\"%Y%m%dT%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a50b4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get boto3 session attributes.\n",
    "account = boto3_session.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = boto3_session.region_name\n",
    "s3_resource = boto3_session.resource(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9002ab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve data bucket name.\n",
    "with open(\"/home/ec2-user/.aiml-bb/stack-data.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    data_bucket = data[\"data_bucket\"]\n",
    "    model_bucket = data[\"model_bucket\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4450469c",
   "metadata": {},
   "source": [
    "## Define and deploy the model <a class=\"anchor\" id=\"define\"></a>\n",
    "To allow for a complete inference pipeline, we chain together the preprocessing, model inference/evaluation, and postprocessing.\n",
    "We will define each of these stages as a Sagemaker `Model` object, then chain them together into an inference pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b457b1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required schema for input into preprocessing step.\n",
    "preprocess_schema_json = json.dumps({\n",
    "    \"input\": [\n",
    "        {\"name\": \"day_of_week\", \"type\": \"int\"},\n",
    "        {\"name\": \"month\", \"type\": \"int\"},\n",
    "        {\"name\": \"op_carrier\", \"type\": \"string\"},\n",
    "        {\"name\": \"origin\", \"type\": \"string\"}, \n",
    "        {\"name\": \"origin_latitude\", \"type\": \"double\"}, \n",
    "        {\"name\": \"origin_longitude\", \"type\": \"double\"},\n",
    "        {\"name\": \"dest\", \"type\": \"string\"}, \n",
    "        {\"name\": \"dest_latitude\", \"type\": \"double\"}, \n",
    "        {\"name\": \"dest_longitude\", \"type\": \"double\"},\n",
    "        {\"name\": \"origin_tmax\", \"type\": \"double\"}, \n",
    "        {\"name\": \"origin_tmin\", \"type\": \"double\"}, \n",
    "        {\"name\": \"origin_prcp\", \"type\": \"double\"}, \n",
    "        {\"name\": \"origin_snow\", \"type\": \"double\"}, \n",
    "        {\"name\": \"origin_snwd\", \"type\": \"double\"},\n",
    "        {\"name\": \"dest_tmax\", \"type\": \"double\"}, \n",
    "        {\"name\": \"dest_tmin\", \"type\": \"double\"}, \n",
    "        {\"name\": \"dest_prcp\", \"type\": \"double\"}, \n",
    "        {\"name\": \"dest_snow\", \"type\": \"double\"}, \n",
    "        {\"name\": \"dest_snwd\", \"type\": \"double\"}\n",
    "    ],\n",
    "     \"output\": {\"name\": \"features\", \"type\": \"double\", \"struct\": \"vector\"}\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19a3edd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing model.\n",
    "preprocess_model = sparkml.model.SparkMLModel(\n",
    "    name=f\"spark-preprocessor-{now}\",\n",
    "    model_data=f\"s3://{model_bucket}/spark-preprocessor/model.tar.gz\",\n",
    "    spark_version=\"2.4\",\n",
    "    sagemaker_session=sm_session,\n",
    "    env={\"SAGEMAKER_SPARKML_SCHEMA\": preprocess_schema_json}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18e2a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inference model.\n",
    "xgb_container_image = sm.image_uris.retrieve(\"xgboost\", region, \"latest\")\n",
    "inference_model = sm.model.Model(\n",
    "    image_uri=xgb_container_image,\n",
    "    model_data=f\"s3://{model_bucket}/sagemaker-xgboost-tuned/model.tar.gz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9e0313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------"
     ]
    }
   ],
   "source": [
    "# Define complete inference pipeline model and deploy.\n",
    "pipeline_model = sm.pipeline.PipelineModel(\n",
    "    name=f\"sm-pipeline-{now}\",\n",
    "    role=role,\n",
    "    models=[\n",
    "        preprocess_model,\n",
    "        inference_model\n",
    "    ]\n",
    ")\n",
    "endpoint_name = f\"pipeline-endpoint-{now}\"\n",
    "pipeline_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd88e00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect a predictor to the endpoint for inference.\n",
    "pipeline_predictor = sm.predictor.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sm_session,\n",
    "    serializer=sm.serializers.JSONSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685ce03e",
   "metadata": {},
   "source": [
    "## Productionize the endpoint <a class=\"anchor\" id=\"prod\"></a>\n",
    "We now ready the model for production by adding autoscaling to ensure high availability and a Model Monitor to continuously track the quality of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ead55ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create necessary resources.\n",
    "autoscaling_client = boto3.client(\"application-autoscaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b0f8bb",
   "metadata": {},
   "source": [
    "### Autoscaling <a class=\"anchor\" id=\"autoscale\"></a>\n",
    "Attach a CPU base autoscaling policy that triggers on high CPU utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a035059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the endpoint for autoscaling.\n",
    "endpoint_resource_id=f\"endpoint/{endpoint_name}/variant/AllTraffic\"\n",
    "register_scalable_target_response = autoscaling_client.register_scalable_target(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=endpoint_resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    MinCapacity=1,\n",
    "    MaxCapacity=2 # Low for testing purposes.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefb17eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a CPU based autoscaling policy.\n",
    "autoscaling_policy_name = f\"CPUUtil-ScalingPolicy\"\n",
    "put_scaling_policy_response = autoscaling_client.put_scaling_policy(\n",
    "    PolicyName=autoscaling_policy_name,\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=endpoint_resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    PolicyType=\"TargetTrackingScaling\",\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        \"TargetValue\": 90.0,\n",
    "        \"CustomizedMetricSpecification\":\n",
    "        {\n",
    "            \"MetricName\": \"CPUUtilization\",\n",
    "            \"Namespace\": \"/aws/sagemaker/Endpoints\",\n",
    "            \"Dimensions\": [\n",
    "                {\"Name\": \"EndpointName\", \"Value\": endpoint_name },\n",
    "                {\"Name\": \"VariantName\",\"Value\": \"AllTraffic\"}\n",
    "            ],\n",
    "            \"Statistic\": \"Average\",\n",
    "            \"Unit\": \"Percent\"\n",
    "        },\n",
    "        \"ScaleInCooldown\": 600,\n",
    "        \"ScaleOutCooldown\": 300\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a876e2d7",
   "metadata": {},
   "source": [
    "### Model monitor <a class=\"anchor\" id=\"monitor\"></a>\n",
    "Create a model quality monitor baselined against the validation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9e9e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model monitor and baseline against validation data.\n",
    "pipeline_model_monitor = sm.model_monitor.DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.4xlarge\",\n",
    "    volume_size_in_gb=30,\n",
    "    max_runtime_in_seconds=1800,\n",
    "    base_job_name=f\"pipeline-model-monitor-{now}\"\n",
    ")\n",
    "pipeline_model_monitor_baseline = pipeline_model_monitor.suggest_baseline(\n",
    "    baseline_dataset=f\"s3://{data_bucket}/preprocessor_output/validation/\",\n",
    "    dataset_format=sm.model_monitor.dataset_format.DatasetFormat.csv(header=False),\n",
    "    output_s3_uri=f\"s3://{model_bucket}/pipeline-model-monitor/{now}/\",\n",
    "    job_name=f\"pipeline-model-monitor-baseline-{now}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bffbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "baseline_job = my_default_monitor.latest_baselining_job\n",
    "schema_df = pd.io.json.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "schema_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df1966",
   "metadata": {},
   "source": [
    "## Testing the endpoint <a class=\"anchor\" id=\"test\"></a>\n",
    "Test the endpoint in a simple application where the flight information is inputted, and a prediction is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893984c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User inputted features.\n",
    "origin = \"JFK\"\n",
    "dest = \"LAX\"\n",
    "carrier = \"B6\"\n",
    "fl_date = \"2022-01-28\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5640332",
   "metadata": {},
   "source": [
    "All code below would be abstracted away from the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e32d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get date attributes.\n",
    "today = dt.datetime.today().replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "fl_datetime = dt.datetime.strptime(fl_date, r\"%Y-%m-%d\")\n",
    "day_of_week = fl_datetime.weekday() + 1\n",
    "month = fl_datetime.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a895a910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get latitude and longitudes of airports.\n",
    "airport_df = pd.read_parquet(f\"s3://{data_bucket}/dl_output/airport_data\")\n",
    "get_iata_geolocation = (\n",
    "    lambda iata: \n",
    "    airport_df.loc[airport_df[\"iata\"]==iata, [\"latitude\", \"longitude\"]].iloc[0]\n",
    ")\n",
    "origin_lat, origin_lon = get_iata_geolocation(origin)\n",
    "dest_lat, dest_lon = get_iata_geolocation(dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9724213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab weather data.\n",
    "forecast_fqdn = \"https://graphical.weather.gov\"\n",
    "get_geolocation_forecast = (\n",
    "    lambda lat, lon:\n",
    "    xml.etree.ElementTree.fromstring(\n",
    "        requests.get(\n",
    "            f\"{forecast_fqdn}/xml/SOAP_server/ndfdXMLclient.php\",\n",
    "            params={\n",
    "                \"lat\": lat, \"lon\": lon,\n",
    "                \"begin\": today.isoformat(), \n",
    "                \"end\": (today + dt.timedelta(days=7)).isoformat(),\n",
    "                \"Unit\": \"m\",\n",
    "                \"maxt\": \"maxt\", \"mint\": \"mint\",\n",
    "                \"qpf\": \"qpf\", \"snow\": \"snow\",\n",
    "                \"product\": \"time-series\",\n",
    "                \"Submit\": \"Submit\"\n",
    "            }\n",
    "        ).content\n",
    "    )\n",
    ")\n",
    "origin_forecast = get_geolocation_forecast(origin_lat, origin_lon) \n",
    "dest_forecast = get_geolocation_forecast(dest_lat, dest_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3c85e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get averages of date values in XML.\n",
    "def get_avg_xml_value(xml_tree, field, datetime=fl_datetime):\n",
    "    # Get date index key.\n",
    "    layout_key = xml_tree.find(f\".//*{field}\").attrib[\"time-layout\"]\n",
    "    \n",
    "    # Find indices of dates matching date in question.\n",
    "    idxs = []\n",
    "    for idx, date in enumerate(xml_tree.findall(f\".//*time-layout/start-valid-time\")):\n",
    "        datetime = dateutil.parser.parse(date.text)\n",
    "        if fl_datetime.strftime(\"%Y-%m-%d\") == datetime.strftime(\"%Y-%m-%d\"):\n",
    "            idxs.append(idx)\n",
    "            \n",
    "    if not idxs:\n",
    "        raise ValueError(\"Date invalid, no data found for field. Possibly too far into the future.\")\n",
    "            \n",
    "    # Data is for different times of day so we take mean.\n",
    "    # Zero is added so we default in case of no data (e.g. with snow).\n",
    "    val_sum = 0.0\n",
    "    for idx, val in enumerate(xml_tree.findall(f\".//*{field}/value\")):\n",
    "        if idx in idxs:\n",
    "            val_sum += float(val.text)\n",
    "            \n",
    "    return val_sum / len(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0115c9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get forecast values and convert to dataset formats.\n",
    "# In NOAA weather data, all values are scaled by 1/10.\n",
    "origin_tmax = 0.10 * get_avg_xml_value(origin_forecast, \"temperature[@type='maximum']\")\n",
    "origin_tmin = 0.10 * get_avg_xml_value(origin_forecast, \"temperature[@type='minimum']\")\n",
    "origin_snwd = 0.10 * get_avg_xml_value(origin_forecast, \"precipitation[@type='snow']\")\n",
    "origin_liquid = 0.10 * get_avg_xml_value(origin_forecast, \"precipitation[@type='liquid']\")\n",
    "\n",
    "dest_tmax = 0.10 * get_avg_xml_value(dest_forecast, \"temperature[@type='maximum']\")\n",
    "dest_tmin = 0.10 * get_avg_xml_value(dest_forecast, \"temperature[@type='minimum']\")\n",
    "dest_snwd = 0.10 * get_avg_xml_value(dest_forecast, \"precipitation[@type='snow']\")\n",
    "dest_liquid = 0.10 * get_avg_xml_value(dest_forecast, \"precipitation[@type='liquid']\")\n",
    "\n",
    "# This snow to liquid ratio is often assumed, however can be inaccurate.\n",
    "# It is suitable for demonstration purposes, but may need more acccurate \n",
    "# inspection in production use cases.\n",
    "snow_to_liquid_ration = 10.0\n",
    "\n",
    "origin_avg = (origin_tmax + origin_tmin) / 2\n",
    "origin_prcp = origin_liquid if origin_avg > 0 else 0\n",
    "origin_snow = 0 if origin_avg > 0 else snow_to_liquid_ration * origin_liquid\n",
    "\n",
    "dest_avg = (dest_tmax + dest_tmin) / 2\n",
    "dest_prcp = dest_liquid if dest_avg > 0 else 0\n",
    "dest_snow = 0 if dest_avg > 0 else snow_to_liquid_ration * dest_liquid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b02d70b",
   "metadata": {},
   "source": [
    "Send data to the endpoint and make the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aa7f57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "payload =  {\"data\": [\n",
    "    day_of_week, month, \n",
    "    carrier, \n",
    "    origin, origin_lat, origin_lon, \n",
    "    dest, dest_lat, dest_lon,\n",
    "    origin_tmax, origin_tmin, origin_prcp, origin_snow, origin_snwd,\n",
    "    dest_tmax, dest_tmin, dest_prcp, dest_snow, dest_snwd\n",
    "]}\n",
    "print(json.dumps(pipeline_predictor.predict(payload), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19716e44",
   "metadata": {},
   "source": [
    "## Cleanup resources <a class=\"anchor\" id=\"cleanup\"></a>\n",
    "Because this is a temporary project, delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795028a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoscaling_client.delete_scaling_policy(\n",
    "    PolicyName=autoscaling_policy_name,\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=endpoint_resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    ")\n",
    "pipeline_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459a5b79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
